{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ac3611-08e7-46ca-a764-05cbf475f700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1\n",
      "Best score: -0.002488725787746017\n",
      "Generation 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hp/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.002488725787746017\n",
      "Generation 3\n",
      "Best score: -0.002488725787746017\n",
      "Generation 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hp/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.002488725787746017\n",
      "Generation 5\n",
      "Best score: -0.0023958079822043897\n",
      "Generation 6\n",
      "Best score: -0.0023958079822043897\n",
      "Generation 7\n",
      "Best score: -0.002327840745916017\n",
      "Generation 8\n",
      "Best score: -0.002327840745916017\n",
      "Generation 9\n",
      "Best score: -0.0022383880206898114\n",
      "Generation 10\n",
      "Best score: -0.0022383880206898114\n",
      "Generation 11\n",
      "Best score: -0.0022383880206898114\n",
      "Generation 12\n",
      "Best score: -0.0022383880206898114\n",
      "Generation 13\n",
      "Best score: -0.0022383880206898114\n",
      "Generation 14\n",
      "Best score: -0.0022383880206898114\n",
      "Generation 15\n",
      "Best score: -0.0022383880206898114\n",
      "Generation 16\n",
      "Best score: -0.0021931213364030934\n",
      "Generation 17\n",
      "Best score: -0.0021931213364030934\n",
      "Generation 18\n",
      "Best score: -0.0021931213364030934\n",
      "Generation 19\n",
      "Best score: -0.002028399145873974\n",
      "Generation 20\n",
      "Best score: -0.002028399145873974\n",
      "\n",
      "Best Parameters Found:\n",
      "Hidden Layer Size: 34\n",
      "Learning Rate: 0.03395949142315878\n",
      "Activation: relu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "\n",
    "# ---------- 1. Generate Simulated Data ----------\n",
    "def generate_synthetic_data(samples=200):\n",
    "    # Simulate features like inlet_temp, feed_flow, air_pressure, etc.\n",
    "    X = np.random.uniform(low=0, high=1, size=(samples, 4))\n",
    "    # Output could represent yield, moisture content, etc.\n",
    "    y = (0.3 * X[:, 0] + 0.5 * X[:, 1] + 0.2 * X[:, 2] + 0.4 * X[:, 3]) + np.random.normal(0, 0.05, size=samples)\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = generate_synthetic_data()\n",
    "\n",
    "# ---------- 2. Define Genetic Algorithm ----------\n",
    "POP_SIZE = 10\n",
    "GENS = 20\n",
    "\n",
    "# Each individual is [hidden_layer_size, learning_rate_init, activation_function_index]\n",
    "def create_individual():\n",
    "    return [\n",
    "        random.randint(5, 50),                          # neurons\n",
    "        10**random.uniform(-4, -1),                    # learning rate\n",
    "        random.choice([0, 1, 2])                       # activation: 0=identity, 1=logistic, 2=relu\n",
    "    ]\n",
    "\n",
    "def decode_activation(index):\n",
    "    return ['identity', 'logistic', 'relu'][index]\n",
    "\n",
    "def fitness(individual):\n",
    "    hidden_layer_size = individual[0]\n",
    "    learning_rate = individual[1]\n",
    "    activation = decode_activation(individual[2])\n",
    "    \n",
    "    model = MLPRegressor(hidden_layer_sizes=(hidden_layer_size,), \n",
    "                         learning_rate_init=learning_rate,\n",
    "                         activation=activation,\n",
    "                         max_iter=500, random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    return -mse  # We want to minimize MSE\n",
    "\n",
    "def selection(population, scores):\n",
    "    sorted_pop = sorted(zip(population, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [ind for ind, score in sorted_pop[:POP_SIZE//2]]\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    cut = random.randint(1, 2)\n",
    "    child1 = parent1[:cut] + parent2[cut:]\n",
    "    child2 = parent2[:cut] + parent1[cut:]\n",
    "    return child1, child2\n",
    "\n",
    "def mutate(individual):\n",
    "    idx = random.randint(0, 2)\n",
    "    if idx == 0:\n",
    "        individual[0] = random.randint(5, 50)\n",
    "    elif idx == 1:\n",
    "        individual[1] = 10**random.uniform(-4, -1)\n",
    "    else:\n",
    "        individual[2] = random.choice([0, 1, 2])\n",
    "    return individual\n",
    "\n",
    "# ---------- 3. Run GA ----------\n",
    "population = [create_individual() for _ in range(POP_SIZE)]\n",
    "\n",
    "for gen in range(GENS):\n",
    "    print(f\"Generation {gen+1}\")\n",
    "    scores = [fitness(ind) for ind in population]\n",
    "    print(\"Best score:\", max(scores))\n",
    "    selected = selection(population, scores)\n",
    "    \n",
    "    children = []\n",
    "    while len(children) < POP_SIZE - len(selected):\n",
    "        p1, p2 = random.sample(selected, 2)\n",
    "        c1, c2 = crossover(p1, p2)\n",
    "        children.extend([mutate(c1), mutate(c2)])\n",
    "    \n",
    "    population = selected + children[:POP_SIZE]\n",
    "\n",
    "# ---------- 4. Get Best Individual ----------\n",
    "best_individual = max(population, key=fitness)\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(\"Hidden Layer Size:\", best_individual[0])\n",
    "print(\"Learning Rate:\", best_individual[1])\n",
    "print(\"Activation:\", decode_activation(best_individual[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee738cec-9897-4a20-9b6c-ef91bacb0a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
